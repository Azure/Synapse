{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Create sample datasets - NYC Yellow Taxi trip records and NOAA ISD Weather - in your data lake\r\n",
        "\r\n",
        "The code of this notebook is based on a sample notebook _\"Using Azure Open Datasets in Synapse\"_ (PySpark version) available in the Knowledge Center of Synapse Studio."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NYC Yellow Taxi trips dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.opendatasets import NycTlcYellow\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "end_date = parser.parse('2018-12-31')\n",
        "start_date = parser.parse('2009-01-01')\n",
        "\n",
        "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
        "df = nyc_tlc.to_spark_dataframe()\n",
        "\n",
        "# TODO: Replace <storage_name> with the name of your Primary ADLS Gen2 storage account name\n",
        "# and <container_name> with the name of the container chosen as the Primary ADLS Gen2 file system\n",
        "path = \"abfss://<container_name>@<storage_name>.dfs.core.windows.net/nycyellowtaxi\"\n",
        "\n",
        "df.write.partitionBy(\"puYear\", \"puMonth\").mode(\"overwrite\").parquet(path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NOAA IDS Weather dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.opendatasets import NoaaIsdWeather\r\n",
        "import pyspark.sql.functions as f\r\n",
        "\r\n",
        "isd = NoaaIsdWeather(start_date, end_date)\r\n",
        "isd_df = isd.to_spark_dataframe()\r\n",
        "\r\n",
        "weather_df = isd_df.filter(isd_df.latitude >= '40.53')\\\r\n",
        "                        .filter(isd_df.latitude <= '40.88')\\\r\n",
        "                        .filter(isd_df.longitude >= '-74.09')\\\r\n",
        "                        .filter(isd_df.longitude <= '-73.72')\\\r\n",
        "                        .filter(isd_df.temperature.isNotNull())\\\r\n",
        "                        .withColumnRenamed('datetime','datetime_full')\r\n",
        "\r\n",
        "columns_to_remove_weather = [\"usaf\", \"wban\", \"longitude\", \"latitude\"]\r\n",
        "weather_df_clean = weather_df.select([column for column in weather_df.columns if column not in columns_to_remove_weather])\\\r\n",
        "                        .withColumn('datetime',f.to_date('datetime_full'))\r\n",
        "\r\n",
        "aggregations = {\"snowDepth\": \"mean\", \"precipTime\": \"max\", \"temperature\": \"mean\", \"precipDepth\": \"max\"}\r\n",
        "weather_df_grouped = weather_df_clean.groupby(\"datetime\", \"year\", \"month\").agg(aggregations)\r\n",
        "\r\n",
        "weather_df_grouped = weather_df_grouped.withColumnRenamed('avg(snowDepth)','avg_snowDepth')\\\r\n",
        "                                       .withColumnRenamed('avg(temperature)','avg_temperature')\\\r\n",
        "                                       .withColumnRenamed('max(precipTime)','max_precipTime')\\\r\n",
        "                                       .withColumnRenamed('max(precipDepth)','max_precipDepth')\r\n",
        "\r\n",
        "# TODO: Replace <storage_name> with the name of your Primary ADLS Gen2 storage account name\r\n",
        "# and <container_name> with the name of the container chosen as the Primary ADLS Gen2 file system\r\n",
        "path_isd = \"abfss://<container_name>@<storage_name>.dfs.core.windows.net/isdweather\"\r\n",
        "\r\n",
        "weather_df_grouped.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(path_isd)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}